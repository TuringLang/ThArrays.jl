var documenterSearchIndex = {"docs":
[{"location":"ad/#Auto-Differentiation-1","page":"AD","title":"Auto Differentiation","text":"","category":"section"},{"location":"ad/#","page":"AD","title":"AD","text":"If you are familiar with PyTorch's AutoGrad, its usage in Julia is very straightforward:","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"julia> a = rand(3, 2);\n\njulia> t = Tensor(a, requires_grad=true)\nPyTorch.Tensor{Float64, 2}:\n 0.1360  0.5980\n 0.5152  0.4439\n 0.6944  0.2513\n[ CPUDoubleType{3,2} ]\n\njulia> s = sum(3 * t)\nPyTorch.Tensor{Float64, 0}:\n7.91639\n[ CPUDoubleType{} ]\n\njulia> ThAD.backward(s)\n\njulia> ThAD.grad(t)\nPyTorch.Tensor{Float64, 2}:\n 3  3\n 3  3\n 3  3\n[ CPUDoubleType{3,2} ]\n","category":"page"},{"location":"ad/#ThArrays.ThAD-1","page":"AD","title":"ThArrays.ThAD","text":"","category":"section"},{"location":"ad/#","page":"AD","title":"AD","text":"We also provide some convenient functions in the module ThArrays.ThAD, of which the most useful two are gradient and forward:","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"julia> a = rand(3, 2);\n\njulia> b = rand(3, 2);\n\njulia> f(x, y) = x^2 + 3x + sin(y) - y\nf (generic function with 1 method)\n\njulia> grads = ThAD.gradient(f, a, b; d=Tensor(ones(3,2)))\n(PyTorch.Tensor{Float64, 2}:\n 4.8802  3.5663\n 3.7516  4.2127\n 3.0235  4.2004\n[ CPUDoubleType{3,2} ]\n, PyTorch.Tensor{Float64, 2}:\n0.01 *\n-4.4241 -18.4686\n -0.1206 -42.0608\n -27.2898 -0.6701\n[ CPUDoubleType{3,2} ]\n)\n\n\njulia> a = rand(3, 2);\n\njulia> b = rand(3, 2);\n\njulia> f(x, y) = sum(2x + 2y)\nf (generic function with 1 method)\n\njulia> y, back = ThAD.forward(f, a, b);\n\njulia> y\n7.9688797507467255\n\njulia> back(1)\n(PyTorch.Tensor{Float64, 2}:\n 2  2\n 2  2\n 2  2\n[ CPUDoubleType{3,2} ]\n, PyTorch.Tensor{Float64, 2}:\n 2  2\n 2  2\n 2  2\n[ CPUDoubleType{3,2} ]\n)\n","category":"page"},{"location":"ad/#ThArrays.TrackerAD-1","page":"AD","title":"ThArrays.TrackerAD","text":"","category":"section"},{"location":"ad/#","page":"AD","title":"AD","text":"With module ThArrays.TrakcerAD, ThArrays provides the ability to do AD using both Tracker.jl and PyTorch backend. That is, you can track some computation with Tracker and track others with PyTorch in one forward:","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"using Tracker: forward, data\n\nusing ThArrays\nusing ThArrays.TrackerAD: _th, _tr\n\na = rand(3, 2)\nb = rand(3, 2)\n\nf1(x, y) = sum(sin.(_th(x)) + sin.(_th(y)))\ny, back = forward(f1, a, b)\nb = data(back(2))\n","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"The function _th is used to convert a Julia array (or TrackedArray) to a Tensor (or TrackedTensor), and _tr is for the conversion in the opposite direction.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"So in the function f1 above, we convert x and y into Tensors immediately after we get them from the arguments, then, all the computation (sin, +, sum) will be tracked by PyTorch.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"Let's see more examples:","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"# sin with tr, (+, sum) wtih th\nf2(x, y) = sum(_th(sin.(x)) + _th(sin.(y)))\n# all with tr(Tracker Backend)\nf3(x, y) = sum((sin.(x)) + (sin.(y)))\n# (sin, +) with th, sum with tr\nf4(x, y) = sum(_tr(sin.(_th(x)) + sin.(_th(y))))\n# (sin, +) with tr, sum with th\nf5(x, y) = sum(_th(sin.(x) + sin.(y)))","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"In f2, sin is called directly on x and y, which are Julia arrays, so sin to tracked by Tracker.jl. Then the results are converted to Tensors by _th, thus + and sum are tracked by PyTorch.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"In f3 there's no conversion by _th or _tr so all the computation are tracked by Trcaker.jl.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"In f4, sin and + are tracked by PyTorch and sum is by Tracker.jl.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"In f5, sin and + are tracked by Tracker.jl while sum is tracked by PyTorch.","category":"page"},{"location":"ad/#","page":"AD","title":"AD","text":"<!– Local Variables: –> <!– mode: markdown –> <!– mode: auto-fill –> <!– End: –>","category":"page"},{"location":"torchscript/#TorchScript-Support:-ThArrays.ThJIT-1","page":"TorchScript","title":"TorchScript Support: ThArrays.ThJIT","text":"","category":"section"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"Currently we have a limit support to the PyTorch JIT (TorchScript):","category":"page"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"using ThArrays\n\nscript = \"\"\"\ndef main(a, b):\n    return a + b\n\"\"\"\ncu = ThJIT.compile(script)\n\nta = Tensor(rand(3, 2))\ntb = Tensor(rand(3, 2))\n\nres = cu.main(ta, tb) # same as ta + tb\n","category":"page"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"In the example above, we compule a piece of TorchScript to a CompilationUnit, then call the main function in it with two Tensors.","category":"page"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"The current limitations includes:","category":"page"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"All arguments we pass to a function must be Tensors;\nThe return value of the called function must be a single Tensor (not list or tuple of Tensors).","category":"page"},{"location":"torchscript/#","page":"TorchScript","title":"TorchScript","text":"<!– Local Variables: –> <!– mode: markdown –> <!– mode: auto-fill –> <!– End: –>","category":"page"},{"location":"reference/#API-Reference-1","page":"Reference","title":"API Reference","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"This page provides a comprehensive reference for ThArrays functionality.","category":"page"},{"location":"reference/#Tensor-1","page":"Reference","title":"Tensor","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"ThArrays.Tensor","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"<!– Local Variables: –> <!– mode: markdown –> <!– mode: auto-fill –> <!– End: –>","category":"page"},{"location":"tensor/#Tensor-1","page":"Tensor","title":"Tensor","text":"","category":"section"},{"location":"tensor/#Tensor-2","page":"Tensor","title":"Tensor","text":"","category":"section"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"ThArrays.Tensor is an array-like data structure, we can create a Tensor instance from a Julia array directly:","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"julia> a1 = rand(3,2)\n3×2 Array{Float64,2}:\n 0.980161  0.612138\n 0.66993   0.564298\n 0.956528  0.620299\n\njulia> t1 = Tensor(a1)\nPyTorch.Tensor{Float64, 2}:\n 0.9802  0.6121\n 0.6699  0.5643\n 0.9565  0.6203\n[ CPUDoubleType{3,2} ]\n","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"In this way, the Tensor and the Julia Array share the underlying data, if you change the elements of one of them, the other one also sees the change:","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"julia> a1[1] = 1\n1\n\njulia> t1[2] = 2\n2\n\njulia> a1\n3×2 Array{Float64,2}:\n 1.0       0.612138\n 2.0       0.564298\n 0.956528  0.620299\n\njulia> t1\nPyTorch.Tensor{Float64, 2}:\n 1.0000  0.6121\n 2.0000  0.5643\n 0.9565  0.6203\n[ CPUDoubleType{3,2} ]\n","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"If you don't want them to share the underlying data, just pass detach=true when constructing Tensors:","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"julia> a2 = rand(3, 2)\n3×2 Array{Float64,2}:\n 0.98454   0.906715\n 0.945537  0.414818\n 0.211776  0.228856\n\njulia> t2 = Tensor(a2, detach=true)\nPyTorch.Tensor{Float64, 2}:\n 0.9845  0.9067\n 0.9455  0.4148\n 0.2118  0.2289\n[ CPUDoubleType{3,2} ]\n\njulia> a2[1] = 1\n1\n\njulia> t2[2] = 2\n2\n\njulia> a2\n3×2 Array{Float64,2}:\n 1.0       0.906715\n 0.945537  0.414818\n 0.211776  0.228856\n\njulia> t2\nPyTorch.Tensor{Float64, 2}:\n 0.9845  0.9067\n 2.0000  0.4148\n 0.2118  0.2289\n[ CPUDoubleType{3,2} ]\n","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"Calling Convert(::Type{Array}, t::Tensor) will convert a Tensor to a Julia array.","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"ThArrays.Tensor also supports many other operations like Julia arrays do, including iteration, indexing, assignment via indexing, concatenating (cat, hcat, vcat), etc.","category":"page"},{"location":"tensor/#The-0-dim-Tensor-1","page":"Tensor","title":"The 0-dim Tensor","text":"","category":"section"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"It's worth noting that, the elements of a Tensor{T,N} are not of type T, they are of type Tensor{T,0} which is a 0-dim tensor. The value of a 0-dim Tensor can be fetch by convert(::Type{T}, t::Tensor{T, 0}) or more simply, t[].","category":"page"},{"location":"tensor/#Tensor-on-GPU-1","page":"Tensor","title":"Tensor on GPU","text":"","category":"section"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"If you installed ThArrays with CUDA support, you can move your Tensors onto GPU by calling to:","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"julia> t1 = Tensor(rand(3, 2))\nPyTorch.Tensor{Float64, 2}:\n 0.6791  0.1224\n 0.0466  0.8784\n 0.7620  0.1394\n[ CPUDoubleType{3,2} ]\n\njulia> on(t1)\nCPU()\n\njulia> t2 = to(t1, CUDA(2))\nPyTorch.Tensor{Float64, 2}:\n 0.6791  0.1224\n 0.0466  0.8784\n 0.7620  0.1394\n[ CUDADoubleType{3,2} ]\n\njulia> on(t2)\nCUDA(2)\n\njulia> t2 * 2\nPyTorch.Tensor{Float64, 2}:\n 1.3582  0.2449\n 0.0931  1.7569\n 1.5240  0.2789\n[ CUDADoubleType{3,2} ]\n","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"As you see, on can be used to determine which device a Thensor is on.","category":"page"},{"location":"tensor/#","page":"Tensor","title":"Tensor","text":"<!– Local Variables: –> <!– mode: markdown –> <!– mode: auto-fill –> <!– End: –>","category":"page"},{"location":"#ThArrays-1","page":"Home","title":"ThArrays","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"ThArrays is a Julia interface for the PyTorch's C++ backend. It aims on bringing the fundamental facilities, e.g., Tensor, AutoGrad, TorchScript, etc., to the Julia ecosystem.","category":"page"},{"location":"#Getting-Started-1","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Install the package by ] add Tharrays, or if you cloned the code repository and intend to build it from source, set the environment variable export THARRAYS_DEV=1 and run ] build ThArrays. The build script will download the libtorch zip file, compile the shared library, and generate many Julia methods in module ThArrays.ThC. Without setting THARRAYS_DEV, the build script will download the pre-built binary library instead of building it locally.\nRun a simple example:\n julia> using ThArrays\n\n julia> t = Tensor( -rand(3, 3) )\n PyTorch.Tensor{Float64, 2}:\n -0.1428 -0.7099 -0.1446\n -0.3447 -0.0686 -0.8287\n -0.2692 -0.0501 -0.2092\n [ CPUDoubleType{3,3} ]\n\n julia> abs(t)\n PyTorch.Tensor{Float64, 2}:\n  0.1428  0.7099  0.1446\n  0.3447  0.0686  0.8287\n  0.2692  0.0501  0.2092\n [ CPUDoubleType{3,3} ]\n\n julia> sin(t)^2 + cos(t)^2\n PyTorch.Tensor{Float64, 2}:\n  1.0000  1.0000  1.0000\n  1.0000  1.0000  1.0000\n  1.0000  1.0000  1.0000\n [ CPUDoubleType{3,3} ]\n\n julia> t\n PyTorch.Tensor{Float64, 2}:\n -0.1428 -0.7099 -0.1446\n -0.3447 -0.0686 -0.8287\n -0.2692 -0.0501 -0.2092\n [ CPUDoubleType{3,3} ]\n\n julia> ThC.abs!(t)\n PyTorch.Tensor{Float64, 2}:\n  0.1428  0.7099  0.1446\n  0.3447  0.0686  0.8287\n  0.2692  0.0501  0.2092\n [ CPUDoubleType{3,3} ]\n\n julia> t\n PyTorch.Tensor{Float64, 2}:\n  0.1428  0.7099  0.1446\n  0.3447  0.0686  0.8287\n  0.2692  0.0501  0.2092\n [ CPUDoubleType{3,3} ]\n\n julia> ThAD.gradient(x->sum(sin(x)+x^2), rand(3,3))\n (PyTorch.Tensor{Float64, 2}:\n  2.3776  1.5465  2.0206\n  1.2542  1.2081  2.1156\n  2.1034  1.1568  2.2599\n [ CPUDoubleType{3,3} ]\n ,)\n\n julia>\n\nRead on the documents to learn more about ThArrays.","category":"page"},{"location":"#Features-1","page":"Home","title":"Features","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"ThArrays provides:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ThArrays.Tensor: PyTorch Tensor as an Array-like data type in  Julia\nThArrays.ThAD: AD using PyTorch C++ backend\nThArrays.TrackerAD: AD using Tracker.jl and PyTorch C++  backend mixed, on your choice\nThArrays.ThJIT: using TorchScript in Julia","category":"page"},{"location":"#The-shared-library-1","page":"Home","title":"The shared library","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"We wrap libtorch to a shared library (libtorch_capi) to expose symbols that can be called by Julia's ccall directly. That shared library depends on nothing but the libtorch C++ library, that is, it does NOT depend on Julia either, so every language or platform who has an FFI facility like Juiia's ccall can use it to wrap a PyTorch library.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The files csrc/torch_capi* are maintianed by this project and they are used to provide consturctors and several crucial functions of the Tensor and Scalar types.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The files csrc/torch_api* are copied from project ocaml-torch (the src/wrapper directory) with a few minor modifications (remove ocaml dependency, add a generic error handling approach, etc.).","category":"page"},{"location":"#The-auto-generated-ThArrays.ThC-module-1","page":"Home","title":"The auto-generated ThArrays.ThC module","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"As we said in the last section, we borrowed some C++ sources from the ocaml-torch project, and these files are auto-generated (by a program in the ocaml-torch project and based on the YAML declaration files, for example the file native_functions.yaml, in the PyTorch project).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"In this project, we use a Julia program, src/thc/thc-generator.jl to generate Julia functions who call the auto-generated C/C++ functions via ccall, and put them into module ThArrays.ThC (src/thc/thc.jl).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Beside the functions in ThArrays.ThC module, we can find the Python API of type Tensor here, and extract a list by running:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"cat tensors.html | perl -n -e 'print \"$1\\n\" if (m{<code class=\"sig-name descname\">(.+)</code>.*x2192; Tensor}i);' | uniq","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The result of this command is saved as python-api-tensor.txt under this directory, if you found any convenient API in it but not in this package, tell us and we can add it in.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Another place to find functions on Tensor is the C++ API document.","category":"page"},{"location":"#Build-with-CUDA-support-1","page":"Home","title":"Build with CUDA support","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"By default, if you install this package using Julia's package manager(Pkg), it only supports Tensor on CPU. But it also supports Tensors on CUDA GPU if you:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"have CUDA installed on your machine\ndownload libtorch with CUDA support and unzip it to the csrc/libtorch directory of this package\nexport THARRAYS_DEV=1\nstart Julia, run ] build ThArrays","category":"page"},{"location":"#","page":"Home","title":"Home","text":"<!– Local Variables: –> <!– mode: markdown –> <!– mode: auto-fill –> <!– End: –>","category":"page"}]
}
